To check how long it takes for a request to go through

index=$cluster_name* account_name=$account_name| eval t=Tt-Tq | timechart span=5m eval(count()/60)
as throughput, p50(t) as p50, p90(t) as p90, p95(t) as p95, p99(t) as p99

If views are building, check if view_update have been bypassed:


acurl https://bm-whisk001.cloudant.com/_node/dbcore@db4.bm-whisk001.cloudant.net/_config | jq .

if not ask ops if they can do it to help the indexing build quickly

how to check for latency for a all  view 
index=smrt003* account_name=smrt-prod5 _view | eval t=Tt-Tq | timechart p50(t) as p50, p90(t) 
as p90, p95(t) as p95, p99(t) as p99


how to check for latency for a particular view 
index=smrt003* account_name=smrt-prod5 "_view/route_within_dates" | eval t=Tt-Tq | 
timechart p50(t) as p50, p90(t) as p90, p95(t) as p95, p99(t) as p99


how to check for latency for all view except a particular view 

index=smrt003* account_name=smrt-prod5 _view NOT "_view/route_within_dates" | eval t=Tt-Tq | 
timechart p50(t) as p50, p90(t) as p90, p95(t) as p95, p99(t) as p99


check as well if there is a spike in database reads 
Read/write load increased exponentially on bm-kaiser007 over the past week. The cluster was not properly configured to
handle such load. This presented as increased latency to clients. The ultimate cause of this latency was the fact that 
the internal IOQ system was artificially limiting the cluster's ability to process read/write work. 
We changed the configuration for read/write requests to bypass the IOQ system entirely, which greatly improved latency 
on the cluster.(CCM cluster)

